setwd("C:/Users/perso/OneDrive/Desktop/Crude")
library(tseries)
library(FinTS)
library(lubridate)
library(forecast)
library(readxl)
library(PerformanceAnalytics)
library(rugarch)
library(car)
Price = read.csv("Crude_Oil.csv")
Price.ts = ts(Price,start = c(2000,1),frequency = 12)
#Test for Arch Effect
ArchTest(Oil.ts)
#Test for Arch Effect
ArchTest(Price.ts)
#Calculating Returns
Returns=CalculateReturns(Price.ts,method="log")
autoplot(Returns, col= "blue")
autoplot(Returns)
Returns
Price.ts
Price.ts= Price.ts$Price
Price.ts = ts(Price,frequency = 12)
Price.ts
Price = as.data.frame(Price)
Price.ts = ts(Price,frequency = 12)
Price.ts
Price.ts = ts(Price$Price,frequency = 12)
Price.ts
#Test for Arch Effect
ArchTest(Price.ts)
#Calculating Returns
Returns=CalculateReturns(Price.ts,method="log")
autoplot(Returns, col="blue")
Returns = Returns[-1]
Returns = ts(Returns)
shapiro.test(Returns)
#Getting the Best Garch model (auto.arima of Garch)
garch(Returns,grad="numerical",trace=FALSE)
#SGARCH model specification and fitting
garchModel= ugarchspec(variance.model=list(model = "sGARCH",garchOrder=c(1,1)),
mean.model = list(armaOrder=c(0,0)),
distribution.model = "std"
)
gModel= ugarchfit(spec=garchModel, data=Returns, solver.control=list(trace = 1))
gModel
#SGARCH model specification and fitting
garchModel= ugarchspec(variance.model=list(model = "sGARCH",garchOrder=c(1,1)),
mean.model = list(armaOrder=c(0,0)),
distribution.model = "norm"
)
gModel= ugarchfit(spec=garchModel, data=Returns, solver.control=list(trace = 1))
gModel
Returns = ts(Returns)
shapiro.test(Returns)
#Getting the Best Garch model (auto.arima of Garch)
garch(Returns,grad="numerical",trace=FALSE)
#SGARCH model specification and fitting
garchModel= ugarchspec(variance.model=list(model = "sGARCH",garchOrder=c(1,1)),
mean.model = list(armaOrder=c(0,0)),
distribution.model = "norm"
)
gModel= ugarchfit(spec=garchModel, data=Returns, solver.control=list(trace = 1))
gModel
#SGARCH model specification and fitting
garchModel= ugarchspec(variance.model=list(model = "sGARCH",garchOrder=c(1,1)),
mean.model = list(armaOrder=c(0,0)),
distribution.model = "ged"
)
gModel= ugarchfit(spec=garchModel, data=Returns, solver.control=list(trace = 1))
gModel
#SGARCH model specification and fitting
garchModel= ugarchspec(variance.model=list(model = "sGARCH",garchOrder=c(1,1)),
mean.model = list(armaOrder=c(0,0)),
distribution.model = "sstd"
)
gModel= ugarchfit(spec=garchModel, data=Returns, solver.control=list(trace = 1))
gModel
checkresiduals(gModel)
residuals=gModel$residuals
str(gModel)
#ARCH model specification and fitting
archModel= ugarchspec(variance.model=list(model = "sGARCH",garchOrder=c(1,0)),
mean.model = list(armaOrder=c(0,0)),
distribution.model = "sstd"
)
archModel= ugarchfit(spec=archModel, data=Returns, solver.control=list(trace = 1))
archModel
#Diagnostic plots and news impact curve
plot(archModel, which=2)
plot(archModel, which=3)
plot(archModel, which=8)
plot(archModel, which=9)
plot(archModel, which=10)
plot(archModel, which=12)
plot(archModel, which=all)
plot(archModel, which=12)
plot(archModel)
knitr::opts_chunk$set(echo = TRUE)
library(tseries)
library(FinTS)
library(lubridate)
library(forecast)
library(PerformanceAnalytics)
library(rugarch)
Price = read.csv("Crude_Oil.csv")
Price = as.data.frame(Price)
Price.ts = ts(Price$Price,frequency = 12)
print(Price.ts)
ArchTest(Price.ts)
Returns=CalculateReturns(Price.ts,method="log")
autoplot(Returns, col="blue")
Returns = Returns[-1]
Returns = ts(Returns)
shapiro.test(Price.ts)
garchModel= ugarchspec(variance.model=list(model = "sGARCH",garchOrder=c(1,1)),
mean.model = list(armaOrder=c(0,0)),
distribution.model = "sstd"
)
gModel= ugarchfit(spec=garchModel, data=Returns, solver.control=list(trace = 1))
gModel
archModel= ugarchspec(variance.model=list(model = "sGARCH",garchOrder=c(1,0)),
mean.model = list(armaOrder=c(0,0)),
distribution.model = "sstd"
)
archModel= ugarchfit(spec=archModel, data=Returns, solver.control=list(trace = 1))
archModel
plot(archModel, which=9)
plot(archModel, which=10)
plot(archModel, which=11)
#EGARCH model specification and fitting
egarchModel= ugarchspec(variance.model=list(model = "eGARCH",garchOrder=c(2,1)),
mean.model = list(armaOrder=c(0,0)),
distribution.model = "sstd"
)
egModel= ugarchfit(spec=egarchModel, data=Returns, solver.control=list(trace = 1))
egModel
#EGARCH model specification and fitting
egarchModel= ugarchspec(variance.model=list(model = "eGARCH",garchOrder=c(1,1)),
mean.model = list(armaOrder=c(0,0)),
distribution.model = "sstd"
)
egModel= ugarchfit(spec=egarchModel, data=Returns, solver.control=list(trace = 1))
egModel
#GJR-GARCH model specification and fitting
gjrgarchModel= ugarchspec(variance.model=list(model = "gjrGARCH",garchOrder=c(1,1)),
mean.model = list(armaOrder=c(0,0)),
distribution.model = "sstd"
)
gjrgModel= ugarchfit(spec=gjrgarchModel, data=Returns, solver.control=list(trace = 1))
gjrgModel
#apGARCH model specification and fitting
aparchModel= ugarchspec(variance.model=list(model = "apARCH",garchOrder=c(1,1)),
mean.model = list(armaOrder=c(0,0)),
distribution.model = "std"
)
apModel= ugarchfit(spec=aparchModel, data=Returns, solver.control=list(trace = 1))
apModel
#Volatility Forecast
vForecast = ugarchforecast(archModel,n.ahead=20)
vForecast
setwd("C:/Users/perso/OneDrive/Desktop/ML/Classification/Decision Tree")
#Import dataset
dataset = read.csv('Social_Network_Ads.csv')
#Splitting into test and train
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased,SplitRatio = 0.75)
trainSet = subset(dataset,split == TRUE)
testSet = subset(dataset,split == FALSE)
#Feature scaling
trainSet[,1:2] = scale(trainSet[,1:2])
testSet[,1:2] = scale(testSet[,1:2])
# Model Fitting
library(rpart)
classifier = rpart(formula = Purchased ~ .,
data = trainSet)
#Predicting the probabilities
ClassPred = predict(classifier,type = 'class', newdata = testSet[-3])
#Predicting the probabilities
ClassPred = predict(classifier, newdata = testSet[-3])
#Predicting the probabilities
ClassPred = predict(classifier, newdata = testSet[-3], type = "class")
ClassPred
#Predicting the probabilities
ClassPred = predict(classifier, newdata = testSet[-3], type = 'Class')
#Predicting the probabilities
ClassPred = predict(classifier, newdata = testSet[-3], type = 'class')
# Creating the confusion matrix
matrix = table(testSet[,3],classPred)
#Predicting the probabilities
class(classifier)
ClassPred = predict(classifier, newdata = testSet[-3], type = 'class')
#Predicting the probabilities
ClassPred = predict(classifier, newdata = testSet[-3])
View(testSet)
View(trainSet)
View(testSet)
#Import dataset
dataset = read.csv('Social_Network_Ads.csv')
#Splitting into test and train
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased,SplitRatio = 0.75)
trainSet = subset(dataset,split == TRUE)
testSet = subset(dataset,split == FALSE)
#Feature scaling
trainSet[,1:2] = scale(trainSet[,1:2])
testSet[,1:2] = scale(testSet[,1:2])
# Model Fitting
library(rpart)
classifier = rpart(formula = Purchased ~.,
data = trainSet)
#Predicting the probabilities
classPred = predict(classifier,type = "class", newdata = testSet[-3])
#Predicting the probabilities
classPred = predict(classifier,newdata = testSet[-3])
#Predicting the probabilities
classPred
#Predicting the probabilities
probPred = predict(classifier,newdata = testSet[-3])
classPred = ifelse(probPred > 0.5,1,0)
classPred
# Creating the confusion matrix
matrix = table(testSet[,3],classPred)
matrix
library(ElemStatLearn)
set = trainSet
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
library(ElemStatLearn)
set = trainSet
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
# Visualising the Training set results
library(ElemStatLearn)
set = trainSet
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
prob_set = predict(classifier, newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Decision Tree (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'dodgerblue', 'salmon'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'dodgerblue3', 'salmon3'))
# Visualising the Test set results
library(ElemStatLearn)
set = testSet
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Decision Tree (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'dodgerblue', 'salmon'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'dodgerblue3', 'salmon3'))
plot(classPred)
text(classifier)
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
View(dataset)
